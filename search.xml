<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hexo建站教程</title>
    <url>/p/3417200a.html</url>
    <content><![CDATA[<p>一直想搭一个博客，奈何一直比较懒。这次历尽千辛万苦，终于将网站搭起来了，把心血历程记录在这儿。使用的方式是hexo+next+github。</p>
<p><strong>hexo</strong>：一个基于nodeJS实现的博客框架。它的最大的作用就是能将 markdown文档自动转化成 html文档。markdown文档在格式上是比较友好的。</p>
<p><strong>next</strong>：hexo里面一个非常流行的主题，比较美观。 </p>
<p><strong>github</strong>：github page是一个静态网页平台。</p>
<p>主要流程就是，hexo把markdown文件转化为html文档，然后上传到GitHub，形成GitHub page。</p>
<h2><span id="安装hexo">安装hexo</span></h2><p>首先安装<a href="https://nodejs.org/en/">nodejs</a>，安装完成后输入<code>npm -v</code>，如果出现版本号，那说明安装成功了 </p>
<p>命令行安装hexo，如果是Windows的话，建议使用<code>git bash</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>

<p> 接下来就可以用hexo来生成博客了，先创建一个文件夹，如<code>myblog</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd myblog</span><br><span class="line">hexo init</span><br><span class="line">hexo generate</span><br><span class="line">hexo server</span><br></pre></td></tr></table></figure>

<p>执行完以后会提示访问 <code>localhost:4000</code>，就可以看到首页了。之后调试的时候会经常用到下面命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo server </span><br></pre></td></tr></table></figure>

<h2><span id="安装next主题">安装next主题</span></h2><p><code>next</code>主题是一个比较美观流行的hexo主题，需要在博客根目录下，即<code>myblog</code>目录，执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;iissnan&#x2F;hexo-theme-next themes&#x2F;next</span><br></pre></td></tr></table></figure>

<p>需要注意的是，next主题新版本和老版本仓库不一样，很多人建议使用新版本，因为新版本集成了很多新功能。不过我觉得老版本的风格比较喜欢，这个看个人喜好了。</p>
<p>老版本：<a href="https://github.com/iissnan/hexo-theme-next">iissnan</a> 最新版本是<code>5.1.4</code>，已经不维护了。</p>
<p>新版本：<a href="https://github.com/theme-next/hexo-theme-next">theme-next</a>持续更新中。</p>
<h2><span id="hexo-next配置">hexo next配置</span></h2><p>主题配置主要参考：<a href="https://www.jianshu.com/p/9f0e90cc32c2">hexo next 主题优化</a></p>
<h2><span id="关联githubio">关联github.io</span></h2><p>建一个同名的仓库，然后在<code>hexo</code>配置文件里面进行设置，比如你的用户名叫<code>zhangsan</code>，那仓库名就叫<code>zhangsan.github.io</code>，hexo关联<code>github.io</code>的方法参考网上教程，这里说下重要的步骤：</p>
<p>在博客目录配置文件<code>_config.yml</code>最后面添加deploy字段如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:  </span><br><span class="line">  type: git  </span><br><span class="line">  repo: git@github.com:UserName&#x2F;Blog.git  </span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>

<p>安装<code>hexo-deployer-git</code>，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<p>部署命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy </span><br></pre></td></tr></table></figure>

<p>这样就可以推送到远程仓库，等待一会儿访问<code>zhangsan.github.io</code> 就可以访问到了</p>
<h2><span id="关联域名">关联域名</span></h2><p>把<code>zhangsan.github.io</code>与域名关联起来，阿里云的域名一年只要一块钱，简直白嫖</p>
<h2><span id="hexo基本使用">hexo基本使用</span></h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo g 或 hexo generate   # 在hexo站点根目录下生成public文件夹</span><br><span class="line">hexo c 或 hexo clean      # 把public文件夹删除</span><br><span class="line">hexo s 或 hexo server     # 在本地启动服务，默认地址为 http:&#x2F;&#x2F;localhost:4000&#x2F;</span><br><span class="line">hexo d 或 hexo deploy     # 部署站点，在本地生成.deploy_git文件夹，并将编译后的文件上传至 GitHub</span><br><span class="line"></span><br><span class="line">hexo new [layout] &lt;title&gt; # 例如hexo new photo “my-first-blog” 会尝试在scaffolds中寻找 </span><br><span class="line">                            photo.md布局，若找到，则根据该布局新建文章；若未找到或指令中未指定该参数</span><br><span class="line">                            ，则使用post.md新建文章。</span><br><span class="line">hexo clean &amp;&amp; hexo g      # 删除，讲source&#x2F;_posts文件夹下的文章源文件删除后，执行该命令</span><br></pre></td></tr></table></figure>



<h2><span id="遇到的一些问题">遇到的一些问题</span></h2><p><a href="https://blog.csdn.net/qq_44852901/article/details/122817214">Archieve 无法打开解决办法</a></p>
<p><a href="https://www.zhihu.com/question/353097489">Cannot GET /about/%20 解决办法</a></p>
<p><a href="https://www.cnblogs.com/Createsequence/p/14150758.html">目录无法跳转</a></p>
]]></content>
      <categories>
        <category>建站</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title>埋点数据上报架构比较</title>
    <url>/p/417edfd3.html</url>
    <content><![CDATA[<h2><span id="前言">前言</span></h2><p>埋点数据是大部分互联网公司数据组的重要数据来源，另一个重要来源是业务数据。从数据量上来讲，埋点数据量级要比业务数据大很多。正是因为有海量的埋点数据，用户行为分析，精准推荐才成为了可能， 本文对目前所见的埋点数据上报架构进行了梳理，并分析其优缺点。</p>
<h2><span id="数据上报架构">数据上报架构</span></h2><h3><span id="第一种">第一种</span></h3><p>这种方式全部埋点数据都打往一个topic，所有的埋点共用一套schema，这个schema中包括了公共字段，以及自定义字段，公共字段又可以分为必选字段和可选字段，某些必选字段是打点的sdk自动获取值，不需要用户去指定值；自定义字段塞在一个map中，并且规定，该map中只能存储&lt;string, string&gt;类型，上报的架构大概如下：</p>
<p><img src="/p/417edfd3/1.jpg"></p>
<p>上报时候所有的数据都会上报到同一个kafka topic中，这个kafka分区会很多，用flink消费该topic落盘到hive，这个flink任务会有很大的并行度，这样一来只用维护一个flink任务就可以，如果新增打点，不用修改该flink任务，当打点数量太多的时候，需要对该kafka topic进行扩容，增加flink任务并行度，扩容时会影响所有的事件上报，罗盘到hive因为是flink任务，所以是实时写入，但是只有该小时全部落盘后才会提交分区，因此相当于小时级别延迟</p>
<p>在实践中，对于推荐，或者一些实时算数场景而言，需要实时拿到所需要的数据，此时这种小时级别肯定不适用，解决方法时从总的kafka根据某些规则向其他topic进行分流（上图中右下部分）分流的topic 可以被flink消费或者订阅到带预聚合的olap引擎中去，或者送入实时模型，提高时效性</p>
<h3><span id="第二种">第二种</span></h3><p>这种方式每个埋点事件都会发送到对应的kafka topic中去，其后会跟一个flink任务订阅该topic，落盘到hdfs，每个事件的schema由用户自己定义，flink任务需要指定相应的schema去解析数据，大致的架构如下：</p>
<p><img src="/p/417edfd3/2.jpg"></p>
<p>每个事件只会上报到一个对应的topic，每个flink程序只消费一个topic，落到hdfs，作为离线原始数据。如果用户需要再去实时处理的话，可以订阅这个kafka，落盘到druid或者ck这种时效性高的计算引擎。这样不必再去维护一套分流程序，不过每次新添加事件之后，需要创建一个topic，同时增加一个flink任务，flink任务维护成本比较高，如果某个事件流量上涨，只用单独修改该事件对应的topic和flink任务即可，不影响其他事件。</p>
<h2><span id="总结">总结</span></h2><table>
<thead>
<tr>
<th>维度</th>
<th>第一种</th>
<th>第二种</th>
</tr>
</thead>
<tbody><tr>
<td>schema</td>
<td>统一的schema</td>
<td>不统一schema</td>
</tr>
<tr>
<td>topic</td>
<td>上报到一个topic</td>
<td>每个单独的topic</td>
</tr>
<tr>
<td>flink</td>
<td>一个flink任务</td>
<td>每个单独的flink任务</td>
</tr>
<tr>
<td>hive</td>
<td>一个hive表</td>
<td>每个事件一张表</td>
</tr>
<tr>
<td>实时消费</td>
<td>需要额外的分流程序</td>
<td>不需要额外的分流</td>
</tr>
<tr>
<td>扩容</td>
<td>会影响所有事件</td>
<td>只会影响单个事件</td>
</tr>
</tbody></table>
<p>一般而言，第一种比较适合于数据量小的场景，第二种适合大数据量场景</p>
<p><strong>tips</strong>:</p>
<ul>
<li><p>一般上报字段建议命名使用下划线命名，避免驼峰命名，由于hive对大小写不敏感，驼峰命名很多时候容易出问题</p>
</li>
<li><p>埋点事件的schema建议尽量简单，避免复杂类型，否则在序列化反序列化过程中会很影响性能。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql同步hive时io问题</title>
    <url>/p/e5358cee.html</url>
    <content><![CDATA[<h2><span id="背景">背景</span></h2><p>最近在做mysql同步hive的工作，之前用的是sqoop，想着sqoop是mr任务，效率应该比较低，所以想用spark jdbc去改造这个流程。使用spark jdbc去测试的时候发现，在表数据量比较多（6000W条）并且mysql表中含有<code>longtext</code>字段时，用spark jdbc读取的时候数据库的io突然上升，触发了dba侧的告警，而用sqoop则顺利同步，没有发现问题，对spark该问题进行了详细排查。</p>
<h2><span id="spark-jdbc-原理">spark jdbc 原理</span></h2><p><a href="https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html">spark jdbc</a>读取的源码可以参考 <a href="https://github.com/KinoMin/bigdata-learning-notes/blob/master/note/spark/Spark%E8%AF%BB%E5%8F%96JDBC%E6%95%B0%E6%8D%AE%E6%BA%90%E4%BC%98%E5%8C%96.md">Spark读取JDBC数据源</a></p>
<p>根据spark jdbc的源码，在读取的时候会走到<code>JdbcRelationProvider.createRelation()</code>中来，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def createRelation(</span><br><span class="line">    sqlContext: SQLContext,</span><br><span class="line">    parameters: Map[String, String]): BaseRelation &#x3D; &#123;</span><br><span class="line">  val jdbcOptions &#x3D; new JDBCOptions(parameters)</span><br><span class="line">  val resolver &#x3D; sqlContext.conf.resolver</span><br><span class="line">  val timeZoneId &#x3D; sqlContext.conf.sessionLocalTimeZone</span><br><span class="line">  &#x2F;&#x2F; 获取schema</span><br><span class="line">  val schema &#x3D; JDBCRelation.getSchema(resolver, jdbcOptions)</span><br><span class="line">  &#x2F;&#x2F; 获取分区信息</span><br><span class="line">  val parts &#x3D; JDBCRelation.columnPartition(schema, resolver, timeZoneId, jdbcOptions)</span><br><span class="line">  JDBCRelation(schema, parts, jdbcOptions)(sqlContext.sparkSession)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3><span id="读取schema">读取schema</span></h3><p>先会获取mysql的schema信息，用于spark生成查询计划，在获取schema的时候，是通过<code>JdbcDialect</code> 的<code>getSchemaQuery</code> 函数，如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def getSchemaQuery(table: String): String &#x3D; &#123;</span><br><span class="line">  s&quot;SELECT * FROM $table WHERE 1&#x3D;0&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>而这里的<code>$table</code>是从配置中的<code>options.tableOrQuery</code>获取的，生成方式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val tableOrQuery &#x3D; (parameters.get(JDBC_TABLE_NAME), parameters.get(JDBC_QUERY_STRING)) match &#123;</span><br><span class="line">  &#x2F;&#x2F; dbtable 参数和 query 参数都设置的情况</span><br><span class="line">  case (Some(name), Some(subquery)) &#x3D;&gt;</span><br><span class="line">    throw new IllegalArgumentException(</span><br><span class="line">      s&quot;Both &#39;$JDBC_TABLE_NAME&#39; and &#39;$JDBC_QUERY_STRING&#39; can not be specified at the same time.&quot;</span><br><span class="line">    )</span><br><span class="line">  &#x2F;&#x2F; dbtable 参数和 query 参数都不存在的情况</span><br><span class="line">  case (None, None) &#x3D;&gt;</span><br><span class="line">    throw new IllegalArgumentException(</span><br><span class="line">      s&quot;Option &#39;$JDBC_TABLE_NAME&#39; or &#39;$JDBC_QUERY_STRING&#39; is required.&quot;</span><br><span class="line">    )</span><br><span class="line">  &#x2F;&#x2F; 只有dbtable 参数</span><br><span class="line">  case (Some(name), None) &#x3D;&gt;</span><br><span class="line">    if (name.isEmpty) &#123;</span><br><span class="line">      throw new IllegalArgumentException(s&quot;Option &#39;$JDBC_TABLE_NAME&#39; can not be empty.&quot;)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      name.trim</span><br><span class="line">    &#125;</span><br><span class="line">  &#x2F;&#x2F; 只有query参数</span><br><span class="line">  case (None, Some(subquery)) &#x3D;&gt;</span><br><span class="line">    if (subquery.isEmpty) &#123;</span><br><span class="line">      throw new IllegalArgumentException(s&quot;Option &#96;$JDBC_QUERY_STRING&#96; can not be empty.&quot;)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      s&quot;($&#123;subquery&#125;) SPARK_GEN_SUBQ_$&#123;curId.getAndIncrement()&#125;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>spark jdbc在配置时候可以有query和dbtable两种配置，我测试中使用的是query配置，因此这里生成的<code>getSchemaQuery</code>实际上是一个子查询，完整的sql如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT * FROM (SELECT id, mid, value, created, started, st_id, pc FROM test_aaa) SPARK_GEN_SUBQ_0 WHERE 1&#x3D;0</span><br></pre></td></tr></table></figure>

<p>这个子查询会在mysql内部生成一个临时表，由于<code>test_aaa</code>表数据量很大而且有<code>longtext</code>字段，所以使用了磁盘临时表的方案，这样就会引起大量写盘的操作，所以导致了io突升</p>
<h3><span id="读取数据">读取数据</span></h3><p>继续查看spark在读取数据的时候，走的是<code>JDBCRDD.compute</code> 方法，其中生成的sql为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val sqlText &#x3D; s&quot;SELECT $columnList FROM $&#123;options.tableOrQuery&#125; $myWhereClause&quot;</span><br></pre></td></tr></table></figure>

<p>这个也是一个子查询的形式，所以对于大表也会有上面的问题。</p>
<h2><span id="sqoop-读取mysql原理">sqoop 读取mysql原理</span></h2><p>为啥<a href="https://sqoop.apache.org/">sqoop</a>很平稳呢，看了下sqoop源码，可以参考<a href="https://wujun234.com/post/Sqoop-yuan-ma-cong-MySQL-dao-ru-dao-Hive/">sqoop源码-mysql2hive</a>，查看发现sqoop在配置是query时候，必须包含<code>$CONDITIONS</code>这个标识，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">If you want to import the results of a query in parallel, then each map task will need to execute a copy of the query, with results partitioned by bounding conditions inferred by Sqoop. Your query must include the token &#96;$CONDITIONS&#96; which each Sqoop process will replace with a unique condition expression. You must also select a splitting column with &#96;--split-by&#96;.</span><br></pre></td></tr></table></figure>

<h3><span id="读取schema">读取schema</span></h3><p>在读取schema的时候，会把<code>$CONDITIONS</code>进行替换，从而避免了子查询，这个在 <code>SqlManager.getColumnTypesForQuery()</code> 方法中，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public Map&lt;String, Integer&gt; getColumnTypesForQuery(String query) &#123;</span><br><span class="line">   &#x2F;&#x2F; Manipulate the query to return immediately, with zero rows.</span><br><span class="line">   String rawQuery &#x3D; query.replace(SUBSTITUTE_TOKEN, &quot; (1 &#x3D; 0) &quot;);</span><br><span class="line">   return getColumnTypesForRawQuery(rawQuery);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，这是将<code>$CONDITIONS</code>替换为了<code>(1 = 0)</code>，这样就不用走子查询</p>
<h3><span id="读取数据">读取数据</span></h3><p>在读取数据的时候也是如此，对<code>$CONDITIONS</code>进行替换，详见<code>DataDrivenDBRecordReader.getSelectQuery()</code> 方法，这儿不再赘述。</p>
<h2><span id="seatunnel-jdbc原理">seatunnel jdbc原理</span></h2><p>另外看了下最近比较火的数据同步工具<a href="https://seatunnel.apache.org/">seatunnel</a></p>
<h3><span id="读取schema">读取schema</span></h3><p>发现其在获取schema的时候在<code>JdbcDialect.getResultSetMetaData()</code>方法中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">default ResultSetMetaData getResultSetMetaData(</span><br><span class="line">        Connection conn, JdbcSourceConfig jdbcSourceConfig) throws SQLException &#123;</span><br><span class="line">    PreparedStatement ps &#x3D; conn.prepareStatement(jdbcSourceConfig.getQuery());</span><br><span class="line">    return ps.getMetaData();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这儿需要注意的是，它采用的是<code>PreparedStatement.getMetaData()</code>来获取查询结果的schema，而sqoop和spark其实是构造一个空查询，然后走的<code>ResultSet.getMetaData()</code>，也就是<code>PreparedStatement.executeQuery()</code>之后得到的<code>ResultSet</code>。我看网上说<code>PreparedStatement.getMetaData()</code> 好像更好，但是mysql对这个支持不太好，我感觉差别应该不大。</p>
<h3><span id="读取数据">读取数据</span></h3><p>如果partition字段没有设置的话，会直接使用传入的<code>query</code></p>
<p>如果设置了partition字段的话，会使用子查询的方式，见<code>JdbcSourceFactory.obtainPartitionSql()</code></p>
<h2><span id="解决思路">解决思路</span></h2><p>对于spark，我们可以参考sqoop的方式，设置一个<code>$CONDITIONS</code>标识将其进行替换，从而绕开子查询，这样避免数据库的io上升</p>
<p>具体地，就是去改写<code>JDBCRDD</code> 以及<code>JdbcDialect</code>两个类就可以绕开</p>
<p>这儿还需要注意一个问题是spark在生成物理执行计划的时候，select columnList 的顺序和之前<code>getSchema</code>获取到的顺序是不一样的，主要原因是它将schema的字段类型存放在了一个set中去用于算子之间传输序列化，而set的顺序就和之前schema的顺序不同了，必须按照这个columnList 的顺序去数据库里读取数据才可以</p>
<p>至于为什么不参考seatunnel的方法，因为它还是存在子查询，在有partition字段的时候还是会有问题</p>
<h2><span id="总结">总结</span></h2><p>可以看到，虽然sqoop是一个很老的项目，而且目前已经停止维护了，但是之前在发展的过程中，针对mysql读取还是做了很多调优的</p>
<p>另外，我之前以为的sqoop效率不高是因为它是mr任务，而spark是内存计算，实际测试下来，两者速度差不多，与mr没太大关系，因为这种etl 任务只有一个map阶段，无法发挥出spark内存计算的优势，spark可以做到不比sqoop慢，但是也快不了多少，好处是与hive表的兼容性更好一些</p>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>数据同步</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>hive数据小写问题</title>
    <url>/p/e836dffa.html</url>
    <content><![CDATA[<h2><span id="背景">背景</span></h2><p>最近在做一批埋点数据flink落hive表的改造，原来这批入库的hive表都是json格式的，很占空间，查询效率也很低，想将其改成parquet格式，降低存储，提高查询效率。在对比数据的时候发现，对于map字段中的key，两边大小写不一致，具体表现为：</p>
<ul>
<li><p>json表用spark查询时，对于map字段，key都是小写，比如实际上报{“userId”:111}，查询出的结果为{“userid”:111}</p>
</li>
<li><p>parquet表用spark查询时，对于map字段，key和实际上报数据都是一致的，都为{“userId”:111}</p>
</li>
</ul>
<h2><span id="排查">排查</span></h2><p>hive表的格式为 <code>ROW FORMAT SERDE &#39;org.openx.data.jsonserde.JsonSerDe&#39;</code></p>
<p>他会使用<a href="https://github.com/rcongiu/Hive-JSON-Serde">Hive-JSON-Serde</a> 去解析，具体获取key的时候会使用<code>org.openx.data.jsonserde.json.JSONObject</code>这个类中的<code>getKey()</code>方法，这里面会默认转成小写，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">private String getKey(String key) &#123;</span><br><span class="line">	if(JSONOptions.globalOptions.isCaseInsensitive) &#123;</span><br><span class="line">	return key.toLowerCase();</span><br><span class="line">&#125; else &#123;</span><br><span class="line">	return key;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其实对于json的解析还有另一种方式 <code>org.apache.hive.hcatalog.data.JsonSerDe&#39;</code> 将hive表的<code>ROW FORMAT SERDE</code>改为这种方式之后查询的数据是有大写key的，但是这种方式的问题在于查询的时候如果遇到错误的json数据，会直接报错退出，而 <code>org.openx.data.jsonserde.JsonSerDe</code> 会忽略错误，所以在实际使用中用后者居多，不过后者存在key转小写的问题</p>
<h2><span id="解决">解决</span></h2><p>由于历史的数据都是用<code>org.openx.data.jsonserde.JsonSerDe</code> 解析，下游都有很多任务，所以最后的解决方案是在写入parquet的时候将map类型的key全部转为小写</p>
<h2><span id="总结">总结</span></h2><ul>
<li><p>JSON格式的hive表查询的时候有坑，需要注意</p>
</li>
<li><p>大数据最好使用下划线命名，禁止驼峰命名，会减少很多问题</p>
</li>
<li><p>正确的不一定是可行的，虽然和实际上报数据保持一致是正确的选择，但是为了兼容历史，我们只能让它继续错下去</p>
</li>
</ul>
]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>json</tag>
      </tags>
  </entry>
</search>
